{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for pytorch 초보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' import '''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m''' init tensor'''\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# tensor = numpy + GPU\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# random\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m rnd_nor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m) \u001b[38;5;66;03m# normal distribustion (mean=0, var=1)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m rnd_uni \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# uniform distribustion (0~1)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(rnd_nor)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "''' init tensor'''\n",
    "# tensor = numpy + GPU\n",
    "\n",
    "# random\n",
    "rnd_nor = torch.randn(2,3) # normal distribustion (mean=0, var=1)\n",
    "rnd_uni = torch.rand(2,3)  # uniform distribustion (0~1)\n",
    "\n",
    "print(rnd_nor)\n",
    "print(rnd_nor * 5)\n",
    "print(rnd_nor + rnd_nor)\n",
    "\n",
    "print(rnd_nor.shape)\n",
    "print(rnd_nor.dtype)\n",
    "\n",
    "\n",
    "# other\n",
    "z = torch.zeros([2,4])\n",
    "o = torch.ones([2,4])\n",
    "e = torch.eye(4)\n",
    "\n",
    "print(z)\n",
    "print(o)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n",
      "tensor([4, 6])\n",
      "tensor(11)\n",
      "tensor(11)\n",
      "tensor(11)\n"
     ]
    }
   ],
   "source": [
    "''' list vs tensor '''\n",
    "lst1 = [1,2]\n",
    "lst2 = [3,4]\n",
    "\n",
    "tor1 = torch.tensor([1,2])\n",
    "tor2 = torch.tensor([3,4])\n",
    "\n",
    "print(lst1 + lst2)      # concat\n",
    "print(tor1 + tor2)      # element-wise sum\n",
    "\n",
    "print(torch.dot(tor1, tor2))        # dot   (= tor1.dot(tor2))\n",
    "print(torch.matmul(tor1, tor2))     # matmul\n",
    "print(torch.matmul(tor1.view(1,-1), tor2)[0])   # view : reshape와 유사, but contiguous() 해야 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0911,  0.2355,  1.3368],\n",
      "        [-0.0104,  0.3437, -1.4669],\n",
      "        [-1.7265, -0.5412, -0.9112],\n",
      "        [ 1.0339,  0.6922, -0.7014]])\n",
      "tensor([-1.7265, -0.5412, -0.9112])\n",
      "tensor([ 0.2355,  0.3437, -0.5412,  0.6922])\n",
      "tensor(-0.5412)\n"
     ]
    }
   ],
   "source": [
    "''' indexing, slicing '''\n",
    "rnd1 = torch.randn(4,3)\n",
    "\n",
    "print(rnd1)\n",
    "print(rnd1[2, :])   # row vec\n",
    "print(rnd1[:, 1])   # col vec\n",
    "print(rnd1[2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 5, 8],\n",
      "        [5, 3, 1]])\n",
      "tensor([[3, 2],\n",
      "        [5, 4],\n",
      "        [7, 8]])\n",
      "tensor([[87, 88],\n",
      "        [37, 30]])\n",
      "tensor([[16, 21, 26],\n",
      "        [30, 37, 44],\n",
      "        [54, 59, 64]])\n"
     ]
    }
   ],
   "source": [
    "''' matmul '''\n",
    "mat1 = torch.tensor([[2,5,8], [5,3,1]])     # (2,3)\n",
    "mat2 = torch.tensor([[3,2], [5,4], [7,8]])  # (3,2)\n",
    "\n",
    "print(mat1)\n",
    "print(mat2)\n",
    "print(torch.matmul(mat1, mat2)) # (2,2)     # matmul = @\n",
    "print(torch.matmul(mat2, mat1)) # (3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.6244)\n",
      "tensor(-0.1354)\n",
      "tensor(0.9132)\n",
      "tensor(0.9556)\n",
      "-1.6243630647659302\n",
      "tensor([[1., 1., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "''' function '''\n",
    "\n",
    "# operator\n",
    "print(torch.sum(rnd1))\n",
    "print(torch.mean(rnd1))\n",
    "print(torch.var(rnd1))\n",
    "print(torch.std(rnd1))\n",
    "\n",
    "print(torch.sum(rnd1).item())   # item 추출 : tensor -> scalar\n",
    "\n",
    "\n",
    "# condition\n",
    "print(torch.where(rnd1>0, torch.ones(rnd1.shape), torch.zeros(rnd1.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "''' numpy <-> tensor '''\n",
    "t1 = torch.ones(3)\n",
    "n1 = t1.numpy()             # tensor -> numpy          \n",
    "t2 = torch.from_numpy(n1)   # numpy -> torch\n",
    "\n",
    "print(type(t1))\n",
    "print(type(n1))\n",
    "print(type(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "''' using GPU '''\n",
    "\n",
    "# check gpu\n",
    "print(torch.cuda.is_available())            # cuda\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "print(torch.backends.mps.is_available())    # mps\n",
    "print(torch.backends.mps.is_built())\n",
    "\n",
    "# torch.tensor([1, 2, 3, 4]).to(\"mps\")        # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1], device='mps:0')\n",
      "tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "# load gpu\n",
    "x_mps = torch.tensor([1,1], device='mps')   # to mps\n",
    "# x_cuda = torch.tensor([1,1]).cuda()       # to cuda\n",
    "x_cpu = x_mps.cpu()                         # to cpu\n",
    "\n",
    "print(x_mps)\n",
    "print(x_cpu)\n",
    "\n",
    "# 다른장치(gpu/cpu) 간 연산 불가\n",
    "# print(x_mps * x_cpu) # RuntimeError: Expected all tensors to be on the same device, but found at least two devices, mps:0 and cpu!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.], requires_grad=True)\n",
      "tensor([1.], requires_grad=True)\n",
      "tensor([3.], grad_fn=<AddBackward0>)\n",
      "tensor([3.], grad_fn=<AddBackward0>)\n",
      "tensor([9.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "''' computational graph (auto-grad) '''\n",
    "in1 = torch.tensor([2.], requires_grad=True)\n",
    "in2 = torch.tensor([1.], requires_grad=True)\n",
    "n1 = in1 + in2\n",
    "n2 = in1 + 1\n",
    "n3 = n1 * n2\n",
    "\n",
    "print(in1)\n",
    "print(in2)\n",
    "print(n1)\n",
    "print(n2)\n",
    "print(n3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.])\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# info\n",
    "print(in1.data)\n",
    "print(in1.grad)\n",
    "print(in1.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.])\n",
      "tensor([3.])\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dd/p4jm7_y14s3bydf6pp07ny2r0000gn/T/ipykernel_7286/2966265506.py:6: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(n1.grad)  # None  UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead.\n",
      "/var/folders/dd/p4jm7_y14s3bydf6pp07ny2r0000gn/T/ipykernel_7286/2966265506.py:7: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(n2.grad)  # None\n",
      "/var/folders/dd/p4jm7_y14s3bydf6pp07ny2r0000gn/T/ipykernel_7286/2966265506.py:8: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(n3.grad)  # None\n"
     ]
    }
   ],
   "source": [
    "# without retain_grad\n",
    "n3.backward(retain_graph=True)  # 연결된 모든 (이전)노드의 grad 계산\n",
    "                                # retain_graph=True : backward() 후에도 그래프 유지, backward() 여러번 반복 가능\n",
    "print(in1.grad)\n",
    "print(in2.grad)\n",
    "print(n1.grad)  # None  UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead.\n",
    "print(n2.grad)  # None\n",
    "print(n3.grad)  # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12.])\n",
      "tensor([6.])\n",
      "tensor([3.])\n",
      "tensor([3.])\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "# with retain_grad\n",
    "n1.retain_grad()\n",
    "n2.retain_grad()\n",
    "n3.retain_grad()\n",
    "\n",
    "n3.backward(retain_graph=True)\n",
    "\n",
    "print(in1.grad)\n",
    "print(in2.grad)\n",
    "print(n1.grad)  # ok\n",
    "print(n2.grad)  # ok\n",
    "print(n3.grad)  # ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.])\n",
      "tensor([0.])\n",
      "tensor([0.])\n",
      "tensor([0.])\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "in1.grad.zero_() # zero_() : (grad 초기화 + inplace) (안하면 중첩)\n",
    "in2.grad.zero_()\n",
    "n1.grad.zero_()\n",
    "n2.grad.zero_()\n",
    "n3.grad.zero_()\n",
    "\n",
    "print(in1.grad)\n",
    "print(in2.grad)\n",
    "print(n1.grad)\n",
    "print(n2.grad)\n",
    "print(n3.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "in1.requires_grad = False   # grad 추적 중지\n",
    "# in1 = in1.detach()\n",
    "\n",
    "print(in1.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "nn.Module의 모든 w,b는 'requires_grad = True' 자동 설정\n",
    "\n",
    "```py\n",
    "# NN format\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Neural Network '''\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# NN\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=100, out_features=100, bias=True)\n",
    "        self.fc1_act = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(100, 10, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc1_act(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc1_act): ReLU()\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# net\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([100, 100])\n"
     ]
    }
   ],
   "source": [
    "# parameters()\n",
    "params = list(net.parameters())\n",
    "\n",
    "print(len(params))      # 4 = 2 layer (weight, bias)\n",
    "print(params[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight\n",
      "fc1.bias\n",
      "fc2.weight\n",
      "fc2.bias\n"
     ]
    }
   ],
   "source": [
    "# named_parameters() : name + parameters\n",
    "for name, param in net.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0320,  0.0784, -0.0139,  ...,  0.0205,  0.0015,  0.0168],\n",
      "        [ 0.0435, -0.0492, -0.0321,  ..., -0.0722, -0.0288,  0.0734],\n",
      "        [-0.0923, -0.0093, -0.0047,  ...,  0.0270, -0.0882,  0.0003],\n",
      "        ...,\n",
      "        [-0.0754,  0.0526,  0.0936,  ..., -0.0461, -0.0519,  0.0386],\n",
      "        [ 0.0818,  0.0842, -0.0654,  ...,  0.0374,  0.0411, -0.0520],\n",
      "        [-0.0357,  0.0919, -0.0352,  ...,  0.0856, -0.0666, -0.0162]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0828,  0.0971, -0.0881,  ...,  0.0926,  0.0925,  0.0142],\n",
      "        [ 0.0976, -0.0198, -0.0380,  ..., -0.0061,  0.0847,  0.0833],\n",
      "        [-0.0458,  0.0956, -0.0406,  ...,  0.0401, -0.0226,  0.0736],\n",
      "        ...,\n",
      "        [ 0.0365,  0.0564, -0.0675,  ..., -0.0030,  0.0017,  0.0307],\n",
      "        [ 0.0414, -0.0438,  0.0053,  ...,  0.0679,  0.0763,  0.0040],\n",
      "        [-0.0880, -0.0620, -0.0475,  ..., -0.0748, -0.0935, -0.0475]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# weight, bias\n",
    "print(net.fc1.weight)   # =     / net.fc1.weight.data : tensor만 추출\n",
    "print(params[0])        # ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8984, grad_fn=<MseLossBackward0>)\n",
      "before: None\n",
      "after: tensor([ 0.0052,  0.0011,  0.0005, -0.0038, -0.0032,  0.0068, -0.0066, -0.0063,\n",
      "        -0.0029,  0.0103, -0.0039, -0.0087,  0.0124,  0.0046,  0.0028,  0.0096,\n",
      "         0.0078, -0.0014, -0.0010,  0.0025,  0.0065,  0.0036, -0.0068,  0.0116,\n",
      "        -0.0136,  0.0035, -0.0096, -0.0021, -0.0001,  0.0092, -0.0056, -0.0018,\n",
      "         0.0164, -0.0109,  0.0013,  0.0085,  0.0038, -0.0019,  0.0067,  0.0029,\n",
      "        -0.0012,  0.0059,  0.0027, -0.0106,  0.0003, -0.0024,  0.0064, -0.0059,\n",
      "         0.0107, -0.0072,  0.0153,  0.0134, -0.0042,  0.0011,  0.0067, -0.0080,\n",
      "        -0.0044, -0.0028,  0.0036,  0.0112, -0.0136, -0.0009,  0.0045, -0.0023,\n",
      "         0.0043, -0.0127,  0.0024, -0.0057,  0.0066,  0.0028,  0.0090, -0.0102,\n",
      "         0.0082,  0.0027,  0.0010,  0.0069,  0.0075, -0.0107,  0.0044,  0.0038,\n",
      "         0.0060,  0.0050,  0.0100, -0.0132,  0.0179,  0.0031,  0.0072, -0.0032,\n",
      "        -0.0071, -0.0034,  0.0077, -0.0022, -0.0097,  0.0123, -0.0049,  0.0015,\n",
      "        -0.0197,  0.0076, -0.0008,  0.0044])\n"
     ]
    }
   ],
   "source": [
    "''' train loop '''\n",
    "batch_size = 10 # batch dim 필수\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "input = torch.randn(batch_size, 100)\n",
    "target = torch.randn(batch_size, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8984, grad_fn=<MseLossBackward0>)\n",
      "before: None\n",
      "after: tensor([ 0.0052,  0.0011,  0.0005, -0.0038, -0.0032,  0.0068, -0.0066, -0.0063,\n",
      "        -0.0029,  0.0103, -0.0039, -0.0087,  0.0124,  0.0046,  0.0028,  0.0096,\n",
      "         0.0078, -0.0014, -0.0010,  0.0025,  0.0065,  0.0036, -0.0068,  0.0116,\n",
      "        -0.0136,  0.0035, -0.0096, -0.0021, -0.0001,  0.0092, -0.0056, -0.0018,\n",
      "         0.0164, -0.0109,  0.0013,  0.0085,  0.0038, -0.0019,  0.0067,  0.0029,\n",
      "        -0.0012,  0.0059,  0.0027, -0.0106,  0.0003, -0.0024,  0.0064, -0.0059,\n",
      "         0.0107, -0.0072,  0.0153,  0.0134, -0.0042,  0.0011,  0.0067, -0.0080,\n",
      "        -0.0044, -0.0028,  0.0036,  0.0112, -0.0136, -0.0009,  0.0045, -0.0023,\n",
      "         0.0043, -0.0127,  0.0024, -0.0057,  0.0066,  0.0028,  0.0090, -0.0102,\n",
      "         0.0082,  0.0027,  0.0010,  0.0069,  0.0075, -0.0107,  0.0044,  0.0038,\n",
      "         0.0060,  0.0050,  0.0100, -0.0132,  0.0179,  0.0031,  0.0072, -0.0032,\n",
      "        -0.0071, -0.0034,  0.0077, -0.0022, -0.0097,  0.0123, -0.0049,  0.0015,\n",
      "        -0.0197,  0.0076, -0.0008,  0.0044])\n",
      "tensor([[-0.0320,  0.0783, -0.0139,  ...,  0.0206,  0.0015,  0.0168],\n",
      "        [ 0.0435, -0.0492, -0.0322,  ..., -0.0722, -0.0288,  0.0734],\n",
      "        [-0.0922, -0.0094, -0.0050,  ...,  0.0270, -0.0883,  0.0003],\n",
      "        ...,\n",
      "        [-0.0754,  0.0524,  0.0937,  ..., -0.0460, -0.0519,  0.0387],\n",
      "        [ 0.0818,  0.0842, -0.0654,  ...,  0.0375,  0.0411, -0.0522],\n",
      "        [-0.0357,  0.0919, -0.0352,  ...,  0.0856, -0.0666, -0.0162]])\n",
      "tensor([ 0.0090, -0.0869,  0.0527, -0.0137,  0.0017,  0.0391, -0.0433, -0.0933,\n",
      "        -0.0780, -0.0476, -0.0695, -0.0271, -0.0467, -0.0227,  0.0823,  0.0992,\n",
      "         0.0660,  0.0350,  0.0760, -0.0245, -0.0491,  0.0079,  0.0548,  0.0252,\n",
      "        -0.0815, -0.0813, -0.0973,  0.0636,  0.0710, -0.0751,  0.0294,  0.0501,\n",
      "         0.0470, -0.0998,  0.0337,  0.0738, -0.0551, -0.0144, -0.0555, -0.0325,\n",
      "         0.0153,  0.0875, -0.0001, -0.0904, -0.0073, -0.0781,  0.0286,  0.0364,\n",
      "        -0.0250,  0.0476,  0.0479, -0.0175,  0.0669,  0.0389,  0.0031,  0.0184,\n",
      "        -0.0706,  0.0652, -0.0189, -0.0876,  0.0141,  0.0559,  0.0059,  0.0689,\n",
      "         0.0791,  0.0334,  0.0219,  0.0701, -0.0707, -0.0229,  0.0179, -0.0167,\n",
      "        -0.0115, -0.0130, -0.0922,  0.0348,  0.0716, -0.0709,  0.0765,  0.0197,\n",
      "        -0.0673,  0.0009, -0.0759,  0.0133,  0.0801, -0.0584,  0.0025, -0.0660,\n",
      "         0.0987, -0.0389, -0.0699, -0.0898, -0.0298,  0.0749, -0.0039, -0.0130,\n",
      "        -0.0136,  0.0103, -0.0689, -0.0963])\n",
      "tensor([[ 3.7147e-02, -6.1096e-02,  6.7289e-02,  1.0849e-02, -8.4708e-02,\n",
      "         -1.2521e-02, -5.9984e-02,  4.4910e-02, -3.3891e-02,  7.9255e-02,\n",
      "          5.5082e-02, -8.1671e-03,  4.9231e-02,  6.9007e-02,  9.8732e-02,\n",
      "         -6.8726e-02,  5.1543e-02,  6.1668e-02, -2.4933e-02, -7.0095e-02,\n",
      "          3.5173e-02, -6.0103e-03,  9.8669e-02, -2.9354e-02,  4.5119e-02,\n",
      "          1.5703e-02, -4.4736e-02, -7.0218e-03,  1.8758e-02,  7.9343e-02,\n",
      "         -8.6631e-02, -7.4674e-02,  5.5562e-02, -5.0154e-02,  6.1858e-02,\n",
      "         -2.1728e-02,  2.3292e-02,  3.8570e-02, -8.8025e-02, -3.5338e-02,\n",
      "          3.5218e-02,  5.9284e-03,  9.5130e-02, -3.5134e-02, -6.1900e-02,\n",
      "          2.6583e-02,  2.2717e-02, -2.3413e-03,  6.4394e-02,  3.2702e-02,\n",
      "          8.6060e-02,  3.7379e-02, -7.5836e-02, -8.4794e-02,  2.3146e-02,\n",
      "         -8.8882e-02, -8.1056e-02, -5.4383e-02,  8.1848e-02, -4.2743e-02,\n",
      "         -9.8264e-02, -1.9317e-02,  3.9839e-02,  1.9076e-02, -6.4251e-02,\n",
      "         -6.7613e-02, -4.7862e-02,  1.1200e-02, -7.3546e-02,  9.9163e-02,\n",
      "          6.9266e-02, -7.5206e-02,  7.2944e-02,  5.2353e-02, -7.8508e-02,\n",
      "          4.3842e-02,  5.4920e-02,  9.1512e-02,  4.5366e-03,  5.4049e-02,\n",
      "          8.7577e-02, -3.9534e-02,  1.0876e-02,  1.7685e-03,  6.4372e-02,\n",
      "         -1.9728e-02, -2.8471e-02, -9.4963e-02, -6.7505e-02, -5.1446e-02,\n",
      "          6.5603e-02, -2.5932e-02, -6.0419e-02,  9.1784e-02, -7.7345e-02,\n",
      "         -6.3187e-02, -5.5047e-02,  8.9082e-02, -6.4337e-02, -7.6031e-02],\n",
      "        [ 2.6791e-02, -6.7034e-02, -5.1826e-02,  1.1831e-02,  5.6135e-02,\n",
      "         -4.6468e-02, -6.6053e-02,  1.8953e-03,  3.9372e-02,  5.9544e-02,\n",
      "         -1.2159e-02, -1.2230e-02,  9.6336e-02, -7.1136e-02,  6.6395e-02,\n",
      "         -9.4384e-02,  5.9970e-02,  7.0436e-02, -6.3646e-03,  2.7379e-02,\n",
      "          6.4357e-02,  3.7615e-02,  8.6992e-02, -6.4649e-02,  1.3032e-02,\n",
      "         -4.5908e-02, -7.4449e-03, -3.5038e-02, -1.3786e-02, -9.7758e-02,\n",
      "          2.1285e-02, -8.8093e-02, -6.7361e-02,  6.7514e-03, -7.8078e-03,\n",
      "         -1.7004e-02, -4.3550e-02, -1.6083e-02,  5.6543e-02,  5.4563e-03,\n",
      "         -8.2603e-02, -3.0364e-02, -5.4198e-02,  5.2262e-02,  9.9109e-02,\n",
      "         -3.2897e-02,  3.1192e-02,  5.4288e-02, -3.6855e-02, -4.2093e-02,\n",
      "          3.7324e-03, -9.5324e-02,  9.3319e-02, -3.4268e-02,  3.4609e-02,\n",
      "          3.5757e-02,  9.3599e-02, -3.7844e-02, -4.6899e-02, -4.0766e-02,\n",
      "          8.7799e-02, -9.7333e-02, -9.6506e-02, -9.7889e-02, -6.2458e-02,\n",
      "          2.1712e-02,  1.5723e-02, -1.0122e-02,  8.0987e-02,  3.2191e-02,\n",
      "         -1.6555e-02,  7.3164e-02,  2.1503e-02, -4.0660e-02,  4.9517e-02,\n",
      "          8.4241e-02,  3.3604e-02,  8.5807e-02, -8.1106e-02, -4.6274e-02,\n",
      "          5.0729e-02, -7.3243e-02, -3.6181e-02, -1.1268e-03, -9.6140e-02,\n",
      "         -5.7067e-03, -2.5491e-02,  7.6397e-02,  9.7878e-02, -3.9547e-02,\n",
      "          9.5837e-02,  9.6184e-02,  1.2006e-02,  5.4197e-03, -9.5560e-02,\n",
      "         -3.8492e-03,  4.6598e-02,  3.3598e-02,  4.8907e-02, -8.8600e-02],\n",
      "        [ 3.3517e-02,  4.5757e-02, -4.2514e-02,  3.1394e-02, -6.5434e-02,\n",
      "         -5.3506e-02, -6.4840e-02,  7.0674e-02,  8.7257e-02,  1.9325e-02,\n",
      "          1.4633e-02, -7.6595e-02, -2.7681e-02,  1.0254e-02, -3.0961e-02,\n",
      "          6.8813e-02,  8.7962e-02, -6.7357e-03, -7.2220e-02, -1.1705e-02,\n",
      "         -8.0262e-02, -3.8147e-02,  8.4664e-02, -4.4397e-02, -8.0367e-02,\n",
      "          6.2866e-02,  1.4552e-02,  9.1547e-02, -6.1379e-02,  4.0367e-03,\n",
      "          3.9127e-02, -5.3857e-02, -8.0619e-02,  8.7569e-02,  9.0921e-02,\n",
      "         -3.2362e-02, -3.0360e-02,  1.3706e-02,  4.0230e-04,  4.5225e-02,\n",
      "          9.3500e-02, -8.0473e-02,  8.7245e-02, -2.5130e-02,  2.4578e-02,\n",
      "         -9.2537e-03, -1.3072e-03, -5.7221e-02, -7.8252e-02,  4.1170e-02,\n",
      "         -1.5283e-02, -1.0400e-02,  2.6243e-02,  7.5628e-02, -7.1507e-02,\n",
      "         -9.1453e-02,  9.5911e-02,  1.2999e-02,  3.8739e-03, -4.7044e-02,\n",
      "         -7.8549e-02, -8.9066e-02,  2.7206e-02,  6.5230e-02, -3.7851e-02,\n",
      "         -8.0983e-02, -7.3164e-03,  4.5631e-02,  3.2424e-02, -9.0946e-02,\n",
      "         -3.3522e-03,  1.9477e-03, -1.3464e-02,  3.5526e-02, -8.7502e-02,\n",
      "          9.7818e-02, -6.2609e-02, -1.4236e-02,  4.8599e-02, -2.6307e-02,\n",
      "         -5.2919e-02, -5.3096e-02,  4.7397e-02,  5.6088e-02,  9.8812e-02,\n",
      "          2.8904e-02, -8.5407e-02,  5.8611e-04, -5.8589e-02, -1.7234e-02,\n",
      "          4.5129e-02, -8.7614e-02, -4.1120e-02,  5.4994e-02,  8.4507e-02,\n",
      "         -3.9759e-02,  7.4990e-03, -6.3310e-02, -4.4575e-02, -5.2865e-02],\n",
      "        [-7.5740e-02, -7.9830e-02, -6.4822e-02, -8.5787e-02, -9.3800e-02,\n",
      "          5.4611e-02,  6.3291e-03,  6.9062e-02,  2.5792e-02, -5.3437e-03,\n",
      "          6.8606e-02,  1.1001e-02, -8.9460e-02,  9.5305e-03,  5.5424e-02,\n",
      "         -1.3644e-02,  3.3484e-02,  2.8656e-02,  9.7792e-02,  2.2029e-02,\n",
      "          5.9649e-02,  4.8593e-02,  8.4868e-02,  2.1528e-02, -7.1847e-02,\n",
      "          6.4159e-02,  1.5498e-02,  8.2885e-02, -9.0377e-02,  1.3587e-02,\n",
      "         -9.6349e-02,  6.4313e-02, -7.6649e-02, -3.8076e-02,  5.7619e-02,\n",
      "          6.5041e-02, -1.2251e-02,  4.6713e-02,  2.7813e-02,  4.7503e-02,\n",
      "         -7.4034e-02, -5.0507e-04, -2.6431e-02, -9.0211e-02, -2.0242e-02,\n",
      "         -6.9790e-03, -7.1979e-02,  7.4117e-03, -9.1978e-02,  9.1141e-02,\n",
      "         -1.4559e-02, -9.0533e-02, -2.2966e-02,  2.0279e-03,  9.5045e-02,\n",
      "         -2.3642e-02, -1.6789e-02,  5.2085e-02, -5.6102e-02, -4.3097e-02,\n",
      "         -9.6843e-02, -8.5612e-02,  9.9459e-02,  3.8727e-02,  1.9378e-02,\n",
      "          5.8608e-02,  6.6743e-02, -9.1623e-02, -4.0942e-02,  8.5007e-02,\n",
      "         -8.7097e-02,  6.1372e-02,  9.2337e-02, -3.1665e-02,  3.4686e-02,\n",
      "         -7.0321e-03,  8.1556e-02,  3.9953e-02, -8.3248e-02,  8.4678e-02,\n",
      "          6.2449e-02,  8.5267e-02,  9.7575e-02, -8.9036e-02,  1.4500e-02,\n",
      "         -5.7388e-02,  4.7708e-02, -7.9806e-02, -9.2776e-02,  1.4821e-02,\n",
      "         -2.4921e-02,  1.6624e-02,  1.3546e-02, -4.4391e-02,  2.1274e-02,\n",
      "         -5.1636e-02,  1.8258e-02, -2.7732e-02,  5.9613e-02, -1.1987e-03],\n",
      "        [-9.2334e-03, -2.9721e-02,  9.4399e-02,  2.4960e-02,  1.3170e-02,\n",
      "         -9.3635e-02,  7.3281e-02,  8.8582e-02,  2.7548e-02, -3.8258e-02,\n",
      "          9.7160e-02,  5.8000e-02, -8.0063e-02, -1.3077e-02,  8.5633e-02,\n",
      "         -8.1775e-02, -6.8015e-02,  8.1466e-02, -6.6229e-02, -1.8341e-02,\n",
      "          6.7000e-02,  3.6028e-02,  2.6931e-02, -4.0636e-02,  7.4117e-02,\n",
      "         -4.7658e-02,  1.1402e-02, -4.5554e-02, -9.5235e-02, -7.8774e-02,\n",
      "          2.3581e-02,  2.6525e-02, -9.6172e-02,  3.7030e-02,  8.5210e-02,\n",
      "         -1.2469e-02,  9.1188e-02,  2.5370e-02, -5.7242e-02, -8.0574e-02,\n",
      "         -3.8377e-03,  6.0310e-03,  4.1851e-02, -2.6410e-02,  6.0298e-02,\n",
      "          6.6345e-02, -9.4089e-02, -9.2936e-02, -3.9421e-02,  7.9422e-02,\n",
      "         -6.5101e-02, -9.8027e-02,  6.4902e-02, -2.9476e-02,  8.9800e-02,\n",
      "         -7.6267e-02,  5.6884e-02, -1.8834e-02, -9.0896e-04, -9.4898e-02,\n",
      "          5.9463e-02,  4.7201e-03,  5.2671e-03,  8.7717e-02, -6.7594e-02,\n",
      "          3.6203e-02,  5.5211e-02,  2.3011e-03, -6.8002e-02,  6.5737e-02,\n",
      "          7.6010e-02,  8.4788e-03, -1.4708e-02,  3.4867e-03, -1.0868e-02,\n",
      "         -6.0870e-02,  5.3266e-02,  8.2654e-02, -5.8640e-02,  6.5923e-02,\n",
      "         -8.2476e-03, -3.3380e-02, -4.5985e-02,  5.2808e-02, -9.4157e-02,\n",
      "         -7.3916e-03,  7.2940e-03, -7.2247e-02,  4.3189e-02,  2.1289e-02,\n",
      "         -7.6154e-02,  1.3612e-02,  4.0510e-02, -5.9283e-02,  4.9811e-02,\n",
      "          5.9823e-02,  9.6078e-02,  5.7697e-02,  3.2995e-02, -2.0721e-02],\n",
      "        [-6.1548e-02, -2.3851e-02, -7.0305e-02,  1.3621e-03,  8.4581e-02,\n",
      "         -4.2683e-02,  4.3836e-02, -2.8984e-02,  8.9898e-02, -3.3498e-02,\n",
      "          8.7928e-02,  7.1484e-02,  6.3491e-02, -2.1625e-02,  7.9464e-02,\n",
      "          2.2713e-02,  5.7596e-02,  4.8949e-02,  4.5980e-02,  2.8183e-02,\n",
      "         -5.5837e-02, -3.4319e-02,  8.5893e-04, -9.4338e-02,  7.3992e-02,\n",
      "          7.5210e-02,  4.5489e-02,  2.8369e-02, -5.6957e-02,  3.8268e-02,\n",
      "         -9.6977e-02,  3.2903e-02, -9.1106e-02, -3.5144e-02,  2.4178e-02,\n",
      "         -9.9402e-02, -6.5987e-02,  4.8742e-02, -1.6798e-02, -9.7574e-02,\n",
      "          7.4227e-02, -8.0793e-02,  4.3933e-02,  2.8809e-02, -8.9365e-02,\n",
      "          2.7277e-02,  8.8605e-03,  3.4750e-02,  5.5239e-02,  2.5021e-02,\n",
      "         -6.3517e-02, -1.1276e-02,  5.9779e-02,  4.9418e-02, -1.2515e-02,\n",
      "         -2.5708e-03, -1.2385e-02,  6.6876e-02, -1.5768e-02, -3.3143e-02,\n",
      "         -6.7659e-02,  8.7676e-02, -3.2070e-02, -3.5164e-02,  7.6545e-02,\n",
      "          1.1889e-02, -5.5544e-02,  3.4610e-02, -5.6732e-02,  5.0270e-02,\n",
      "         -8.4474e-03,  7.7980e-02,  6.6457e-02, -1.8108e-02,  3.7101e-02,\n",
      "         -9.8568e-02,  3.5753e-02, -7.0993e-02, -2.7771e-02,  1.7609e-02,\n",
      "         -4.5353e-03,  8.2984e-02, -3.2524e-02, -1.0955e-02, -8.9770e-02,\n",
      "         -7.8079e-02, -2.9680e-02,  7.7508e-03, -7.2964e-02, -3.5890e-02,\n",
      "          6.7900e-02, -1.2709e-02,  4.1670e-02,  1.2335e-03,  4.1774e-02,\n",
      "         -5.7817e-03,  1.0697e-03,  2.4087e-02, -2.1632e-02,  7.2325e-02],\n",
      "        [-6.0794e-02,  6.2706e-03, -3.7927e-02, -5.8523e-02, -8.3760e-02,\n",
      "          7.1501e-03,  1.4184e-02,  3.3864e-03,  3.5170e-02, -8.1987e-02,\n",
      "          6.3427e-02, -8.4554e-02,  4.8988e-02,  6.5920e-02,  3.3911e-02,\n",
      "          4.3661e-02,  4.7052e-02,  2.1593e-02, -4.7704e-02, -9.3832e-02,\n",
      "          5.8158e-02,  7.1341e-02, -6.5156e-02, -3.8102e-02, -9.4367e-02,\n",
      "         -4.4815e-02,  9.7565e-02, -4.1953e-02,  9.8819e-02,  9.6967e-02,\n",
      "         -5.1287e-02,  1.5364e-02, -7.5869e-02,  9.9612e-02, -7.9859e-02,\n",
      "         -7.1045e-02, -3.5765e-02,  9.3585e-02,  1.8429e-02, -9.0337e-02,\n",
      "         -5.4786e-02,  8.2655e-02, -3.0082e-02, -4.4599e-02, -5.7985e-03,\n",
      "          5.8362e-02, -2.1352e-02,  7.0656e-02, -8.1117e-02, -5.9097e-02,\n",
      "          8.3725e-02, -9.7013e-03,  9.4874e-02, -5.2184e-02,  9.1899e-02,\n",
      "         -9.0393e-02,  8.2522e-02,  8.9359e-02,  2.5943e-02,  2.7042e-02,\n",
      "         -6.1901e-02, -9.0691e-02, -7.8968e-02,  9.7654e-02,  1.7132e-03,\n",
      "          2.0107e-02,  6.0614e-02,  5.2715e-02,  8.3339e-02,  8.8808e-02,\n",
      "         -7.9303e-02, -6.3896e-02, -3.2644e-02, -4.1386e-03,  5.8176e-02,\n",
      "          7.4580e-02,  7.9019e-02, -4.7014e-02, -5.9053e-02,  9.8606e-02,\n",
      "          2.8809e-02, -6.5861e-02,  4.8822e-02,  2.4567e-02, -5.0537e-02,\n",
      "         -7.0731e-02, -9.4118e-02,  7.8482e-04, -2.5197e-05,  9.3807e-02,\n",
      "          1.9344e-02, -8.7150e-02,  3.8561e-02, -8.2153e-04,  5.2306e-02,\n",
      "          4.0061e-02,  3.7081e-02, -6.9203e-02, -3.1559e-02,  8.9953e-02],\n",
      "        [-6.2711e-02,  4.0716e-02,  4.4869e-04,  8.7447e-02, -3.3776e-02,\n",
      "          6.8337e-02, -4.7944e-02,  2.3871e-02, -6.2623e-02, -3.3834e-02,\n",
      "          5.9074e-02,  5.1918e-02, -6.1844e-02,  6.2906e-02, -8.2473e-02,\n",
      "         -9.7265e-02, -6.5870e-02,  7.4356e-02, -2.4249e-02, -9.5257e-02,\n",
      "         -7.3415e-03,  2.4363e-03,  8.4343e-02,  9.5812e-02,  5.2130e-03,\n",
      "          5.3926e-02,  1.0017e-01, -7.7454e-02,  8.2644e-02, -1.9496e-02,\n",
      "          2.1106e-02, -8.9287e-02, -3.6237e-02, -3.3969e-02,  6.2410e-02,\n",
      "         -9.0764e-02,  7.3418e-02, -7.4351e-03,  2.1226e-02,  7.2992e-02,\n",
      "          2.8626e-02, -9.9253e-03,  9.4461e-02,  6.4958e-02,  3.9046e-03,\n",
      "          6.2859e-02, -6.1408e-02,  8.0803e-02,  5.9681e-02, -3.9326e-02,\n",
      "         -2.7344e-02, -3.7947e-05,  7.6197e-02, -1.0071e-02,  4.8701e-02,\n",
      "         -9.6011e-02, -7.2593e-02,  6.5858e-02, -2.5353e-02, -7.4006e-02,\n",
      "          6.4516e-02,  1.8327e-02,  7.6998e-02,  3.5051e-03, -2.5522e-02,\n",
      "         -1.9090e-02, -5.6069e-02,  5.8772e-02, -8.3871e-02,  4.2557e-02,\n",
      "         -6.1819e-02, -3.7149e-02,  4.9802e-03, -8.8503e-02, -3.8231e-03,\n",
      "          3.7667e-02,  4.7735e-02,  2.2646e-02, -1.6187e-02, -5.1453e-02,\n",
      "         -1.6651e-02, -6.4726e-03,  7.0123e-02,  9.0181e-02,  2.2074e-02,\n",
      "          9.1573e-02, -1.7875e-02, -7.3136e-02,  9.7323e-02, -6.4822e-02,\n",
      "          2.7261e-02,  7.8174e-02,  4.1878e-02, -3.9811e-02,  7.9991e-03,\n",
      "          4.7134e-02,  7.8077e-02,  8.3727e-03, -1.2046e-02,  3.4090e-02],\n",
      "        [ 3.7296e-02, -2.6085e-02,  5.6690e-02, -8.8969e-02, -1.6121e-02,\n",
      "         -9.4536e-02, -2.4286e-02, -1.4947e-02,  8.6394e-03, -5.7153e-02,\n",
      "          3.5120e-03, -4.1648e-02, -9.5781e-02, -1.0853e-02, -3.3796e-02,\n",
      "         -1.7270e-02,  9.5121e-02, -3.6376e-02, -6.1599e-02, -3.5434e-02,\n",
      "          9.5797e-03,  2.9232e-02,  5.9504e-02,  9.5887e-02,  8.3787e-02,\n",
      "         -1.2444e-02,  1.6916e-02,  8.3301e-02, -3.6414e-02,  8.8655e-02,\n",
      "          6.4453e-02,  9.5096e-02, -1.1638e-02,  6.3252e-04, -1.0505e-02,\n",
      "          5.9995e-02, -4.8420e-02,  5.5468e-02, -9.8025e-02, -3.4188e-02,\n",
      "         -3.1274e-02,  4.0000e-02,  4.2313e-02, -9.7896e-02,  7.8346e-02,\n",
      "          8.6089e-02, -3.1613e-02,  1.3830e-02,  1.1536e-02,  1.7350e-02,\n",
      "         -1.8971e-02,  2.2258e-02, -1.9137e-02, -6.3540e-02,  9.2665e-03,\n",
      "          5.3191e-02,  4.0958e-02,  8.9779e-02,  6.1576e-02,  1.9053e-02,\n",
      "         -1.7475e-02,  3.8963e-02,  2.6401e-03,  4.5997e-02,  1.2670e-03,\n",
      "          1.6997e-02, -6.4996e-02, -8.1104e-02,  2.0203e-02, -6.4718e-02,\n",
      "          7.5470e-02,  9.3821e-02,  2.7530e-02,  1.5734e-02, -8.3454e-02,\n",
      "         -6.8705e-02,  1.5505e-02, -7.6617e-02,  3.2688e-02, -5.2287e-02,\n",
      "          4.5716e-02,  9.5309e-02,  3.2435e-02, -7.4367e-02, -3.6603e-02,\n",
      "         -5.0534e-02, -9.1693e-02, -6.4821e-02, -9.0552e-02,  9.4889e-02,\n",
      "         -1.7176e-02,  5.4722e-02, -5.1593e-02,  8.3949e-02, -9.2849e-02,\n",
      "         -5.5064e-02,  4.7329e-02,  8.1184e-02, -1.1647e-02, -4.4752e-02],\n",
      "        [ 7.4925e-02,  5.9637e-02, -9.2849e-02, -1.9130e-02, -4.0492e-02,\n",
      "          6.0512e-02, -4.5577e-02,  7.4207e-02, -7.4321e-02,  5.9835e-02,\n",
      "         -5.3122e-02, -6.9825e-02, -6.9347e-02,  2.4901e-02,  6.7540e-03,\n",
      "         -6.9463e-02,  1.5661e-03,  9.8343e-02, -4.4167e-02,  3.6224e-02,\n",
      "          3.0662e-02,  1.6962e-02, -8.9910e-02,  9.2450e-02, -8.7142e-02,\n",
      "         -1.3428e-02, -7.8886e-02, -8.0405e-02, -9.8258e-03, -9.0333e-02,\n",
      "         -7.9017e-02,  7.8930e-02, -4.8183e-02, -9.5678e-03,  2.5837e-02,\n",
      "         -2.2481e-03,  7.4812e-02, -7.5396e-03, -9.8935e-03,  2.3079e-02,\n",
      "         -8.2147e-02,  6.8395e-02,  7.5963e-02, -6.0524e-02,  6.9577e-02,\n",
      "          8.3272e-02, -2.5279e-02,  8.0424e-02,  9.0107e-02,  5.1136e-02,\n",
      "         -5.3727e-02, -9.5676e-03, -3.9098e-02,  6.8801e-02,  5.8224e-02,\n",
      "         -3.8232e-03,  8.2727e-02, -7.0267e-02, -7.5175e-02, -3.2357e-02,\n",
      "         -2.7120e-02, -9.1123e-02, -7.4053e-02, -5.9598e-02,  8.2562e-02,\n",
      "          6.4851e-02, -5.8922e-02,  3.3401e-02, -4.1289e-02, -4.7084e-02,\n",
      "          9.5959e-02, -4.2307e-02,  6.3427e-02,  5.2919e-02,  3.1088e-02,\n",
      "          1.7570e-02,  5.9209e-02,  2.3072e-03, -2.6142e-02, -2.1107e-02,\n",
      "         -7.4174e-02,  4.5823e-03,  1.4853e-02,  8.4661e-02, -2.5355e-02,\n",
      "          5.6252e-02, -2.7039e-02, -1.3720e-02,  6.9929e-02, -2.0925e-02,\n",
      "         -9.4611e-02,  2.9753e-02, -1.2339e-03, -6.3123e-02, -5.4129e-02,\n",
      "          9.7933e-02,  6.0248e-02, -4.8054e-02, -4.1667e-02, -2.3472e-02]])\n",
      "tensor([ 0.0683, -0.0557, -0.0225, -0.0253,  0.0835,  0.0953, -0.0561, -0.0100,\n",
      "        -0.0288, -0.0106])\n"
     ]
    }
   ],
   "source": [
    "''' without optimizer '''\n",
    "\n",
    "# iter\n",
    "#   forward\n",
    "#   backward\n",
    "#       net.zero_grad()\n",
    "#       loss.backward()\n",
    "#   step\n",
    "\n",
    "# forward\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n",
    "\n",
    "# backward\n",
    "net.zero_grad()\n",
    "print('before:', net.fc1.bias.grad) # None\n",
    "loss.backward()\n",
    "print('after:', net.fc1.bias.grad)  # grad가 생김\n",
    "\n",
    "# step\n",
    "learning_rate = 0.01\n",
    "for param in net.parameters():\n",
    "    print(param.data.sub_(param.grad.data * learning_rate)) # sub()=빼기, sub_()=빼기+inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.0942e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0579e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0221e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.9864e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.9514e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.9164e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8820e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8478e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8138e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7801e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7466e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7138e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6811e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6485e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6163e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.5847e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.5528e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.5215e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.4905e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.4596e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.4291e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.3989e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.3691e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.3391e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.3099e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.2804e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.2517e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.2228e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1945e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1667e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1385e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1109e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.0834e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.0563e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.0293e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.0027e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9763e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9503e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9242e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8984e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8729e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8475e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8226e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7974e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7729e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7487e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7245e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7004e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6767e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6528e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6296e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6065e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5836e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5608e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5384e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5160e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4938e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4719e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4500e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4285e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4074e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3861e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3651e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3442e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3235e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3032e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2828e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2628e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2432e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2234e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2038e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1844e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1652e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1460e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1273e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1084e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0901e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0718e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0534e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0354e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0175e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9999e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9823e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9648e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9476e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9304e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9136e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8967e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8801e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8637e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8472e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8310e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8150e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7990e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7834e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7677e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7522e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7366e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7214e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7063e-07, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "''' with optimizer '''\n",
    "\n",
    "num_epochs = 200\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    # forward\n",
    "    output = net(input)\n",
    "    loss = criterion(output, target)\n",
    "    print(loss)\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()   # = net.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # step\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
